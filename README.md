# GenCheckAiModel 

*Проект подготовлен с использованием изображений, сгенерированных моделью Yandex Art в приложении Шедеврум. Изображения предоставлены исключительно для исследовательских целей данного проекта выпускнику Яндекс Лицея и не предназначены для публичного использования.*

[Telegram бот](https://github.com/dimong4/GenCheckAiBot)

[Kaggle](https://www.kaggle.com/models/dimong4/gencheckai/)

**Стек:** Python, PyTorch, Transformers (Hugging Face), CatBoost, Scikit-learn, Matplotlib.

# Использованные данные 
Сформирован агрегированный датасет объемом **104 700 изображений**.
*   **Реальные данные:** Фотографии из бенчмарка [ImageNet](https://www.image-net.org/).
*   **Российский ИИ (Custom):** Изображения, сгенерированные специально для проекта через модели **Kandinsky** и **YandexART**. Для них был разработан собственный генератор промптов на базе классов ImageNet и выполнена адаптация (перевод) описаний на русский язык.
*   **Зарубежный ИИ (GenImage):** Готовые наборы данных из бенчмарка [GenImage](https://github.com/GenImage-Dataset/GenImage) (Midjourney, Stable Diffusion, BigGAN, VQBM, Wukong, ADM, Glide).

# Исследовательская часть: эволюция модели

## 1. От классических CNN к Vision Transformers
Изначально протестированная архитектура **ResNet-50** показала высокую точность на знакомых данных, но недостаточную обобщающую способность. Для улучшения анализа я добавил **Swin Transformer (Swin-T)**.
*   **Результат:** За счет иерархической структуры и анализа глобальных связей в пикселях, трансформер лучше выявляет композиционные и геометрические ошибки ИИ, которые «пропускают» обычные сверточные сети.

## 2. Переход к метрическому обучению (Metric Learning)
Чтобы сделать систему устойчивой к новым версиям генераторов, я реализовал переход к **Contrastive Learning**.
*   **Цель:** Обучить нейросеть выстраивать пространство признаков так, чтобы «почерк» реальных фото находился в одном плотном кластере, а генераций — в другом, независимо от конкретной модели.
*   **Реализация:** Использована функция потерь **Triplet Margin Loss** и алгоритм **Triplet Margin Miner**. Модель обучается фиксировать саму структуру данных (шумы, специфические искажения рендеринга), создавая надежный 128-мерный эмбеддинг.


## 3. Zero-shot CLIP
Была проверена гипотеза использования модели **CLIP** в чистом виде для детекции через текстовые триггеры (сравнение близости к понятиям "photo" и "ai generated").
*   **Результат:** Точность составила всего **~73%**. CLIP часто ошибается на качественных студийных фото, принимая их за ИИ из-за идеального освещения.
*   **Решение:** CLIP стал важной частью ансамбля.
![](other/Confusion_Matrix_CLIP.png)

# Финальный результат: Feature Fusion + CatBoost
Итоговое решение объединяет признаки от трех независимых экстракторов (ResNet-Metric, Swin-T, CLIP). Они конкатенируются в один **общий вектор высокой размерности**. 

Этот вектор учитывает изображение на трех уровнях:
1.  **Пиксельный** (локальные артефакты);
2.  **Структурный** (геометрия и глобальный контекст);
3.  **Логический** (семантическая корректность).

Для финальной классификации выбран **CatBoost**, который эффективнее нейросетей находит нелинейные зависимости в объединенных признаках.

**Метрики ансамбля:**
*   Accuracy — **0.98**
*   Weighted F1-Score — **0.9758**
*   ROC-AUC — **0.9973**

![](other/Predicted_Probability.png)
